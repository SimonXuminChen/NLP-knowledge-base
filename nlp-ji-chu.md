# NLP基础

## What is NLP？

NLP = NLU+NLG

即$$Natural Language Processing = Natural Language Understanding + Natural Language Generation$$

自然语言处理的四个层面

* Semantic语义
* Syntax句法
* Morphology单词
* Phonetics声音

## Language Model 语言模型

**本质：**对一段自然语言的文本进行预测概率的大小。语言模型可以提供词表征和单词序列的概率化表示。

因为语言文字是人类认知过程中产生的高层次认知抽象实体，所以我们需要将其转换成计算机可以理解的方式，这个过程称之为文本表示。

文本表示的种类有三种： 1. 离散表示（统计语言模型）：one-hot, 词袋模型（bag-of-words\), tf-idf,n-gram 2. 分布式表示:共现矩阵 3. 神经网络表示：NNLM, word2vec, glove, Elmo,BERT

给定一段话$$S=w_1w_2w_3... P(S)=P(w_1,w_2,w_3...)=P(w_1)P(w_2,w_3...|w_1)P(w_3...|w_1,w_2)...$$

但是这样的模型有很严重的问题：

1. 很多词语同时出现的可能性太多，无法估算\(**自由参数数目**\)；
2. 对非常多词语的组合在语料库中并未出现，这导致了概率可能为0，使数据稀疏严重\(**数据稀疏性**\)。

为此引入了马尔科夫假设，每一个词出现的概率只与前一个词或前几个有限的词有关。



## Statistical Language Model 统计语言模型

> 统计语言模型采用**最大对数似然**来作为目标函数，目标是求解每一个条件概率的值。

### n-gram

N-gram语言模型的目标跟传统统计语言模型一致，都是求解每一个条件概率的值，简单计算N元语法在语料中出现的频率，然后归一化。

1. **unigram:**如果每个词都是独立的，那么我们称之为一元语言模型\(考虑不到两个词语之间的关系搭配\)

   $$P(S)=P(w_1)  P(w_2)  P(w_3)...$$ 

2. **bi-gram:**如果每个词都只与它的前一个词有关，那么我们称之为二元语言模型

   $$P(S)=P(w_1)P(w_2|w_1)P(w_3|w_2)...$$ 

3. **tri-gram:**如果每个词都只与它的前两个词有关，那么我们称之为三元语言模型

   $$P(S)=P(w_1)P(w_2|w_1)P(w_3|w_2,w_1)...$$ 



> 一般来说，N元模型就是假设当前词的出现概率只与它前面的N-1个词有关，且对N&gt;1的模型，会加上句首开头标识和句末标识。

从模型的效果来看，理论上N的取值越大，效果越好。但随着N取值的增加，效果提升的幅度是在下降的。同时还涉及到一个可靠性和可区别性的问题，参数越多，可区别性越好，但同时单个参数的实例变少从而降低了可靠性。

* N-gram 解决了自由参数数目的问题。
* 可以通过平滑化解决数据稀疏性的问题，如加一平滑\(拉普拉斯平滑\)

**词向量:**用来表示词的向量，也可以被认为是**词的特征向量或表征** **词嵌入:**把词语映射为实数域向量的技术。将one-hot的词向量嵌入到一个低维空间 **词干化（stemming\):**就是去除词缀得到词根的过程。

### One-hot 独热编码

将词表示为只含有0-1的向量，维度与词语数\|V\|一致，当前表示的词语的值为1，其他为0，比如一个句子含有100个唯一且不相同的词，那么这个词向量的维度是100，若表示第3个词，则这个词向量只有第三维度是1，其他均为0.

缺点:当文本数大的时候，one-hot向量表示的维度就变得很大。

### Bag-of-words 词袋模型

词袋模型就是将所有词语装进一个袋子里，不考虑其词法和语序的问题，即每个词语都是独立的。即通过一个向量列表示一个句子，每一位表示一个单词，每一位的值表示该单词出现的频率（次数）。

缺点：并不考虑语序关系。

### TF-IDF

TF-IDF是一种用于信息检索与数据挖掘的常用加权技术，常用于挖掘文章中的关键词，而且算法简单高效，常被工业用于最开始的文本数据清洗。一个词语在一篇文章中出现次数越多, 同时在所有文档中出现次数越少, 越能够代表该文章.

* tf\(Term Frequency\)表示词频，表示某个单词在该文本中出现的次数。 $$TF = (\frac{词语i在文本j中出现的次数}{词语i在文本库中出现的总次数})\$$

  注释：有时候，tf也用 $$\frac{词语i在文本j中出现的次数}{文本j中出现最多的词语频率}$$ 来表示

* idf\(Inverse Document Frequency\)：逆文本频率，如果包含词条 t的文档越少,IDF越大，则说明词条具有很好的类别区分能力。 文档频率: df = 该词在所有文本中\(corpus\)出现的次数，比如文本库中共有4个文本，其中两个文本出现了”love“这个词，所以df\("love"\)=2 $$IDF = log(\frac{N}{df+1}) \ (N是指文本库中的总文本数)$$ $$TF-IDF = TF*IDF$$

优点:简单快速，而且容易理解。

缺点:有时候用词频来衡量文章中的一个词的重要性不够全面，有时候重要的词出现的可能不够多，而且这种计算无法体现位置信息，无法体现词在上下文的重要性。

### 共现矩阵

计算两个token之间共同出现的频率



## Neural Network Language Model

> NNLM一般用**最大对数似然**或**KL散度**作为损失函数。

比较复杂，未考虑上下文顺序关系，将前w-1个词当做输入。计算量大，训练效果低。

